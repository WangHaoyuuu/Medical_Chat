{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŒ…å«ä¸‰ç§å¤§æ¨¡å‹apiè°ƒç”¨çš„æµ‹è¯•\n",
    "\n",
    "# åŒä¸€çš„å‡½æ•°è°ƒç”¨\n",
    "# create_modelname_request_message()\n",
    "# fetch__modelname_model_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åƒå¸†å¹³å° - æ–‡å¿ƒä¸€è¨€\n",
    "# %pip install qianfan==0.3.9\n",
    "import qianfan\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from typing import Dict, List\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "def create_wenxin_request_messages(prompt: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    æ„é€ ç”¨äºæ–‡å¿ƒæ¨¡å‹çš„è¯·æ±‚å‚æ•° `messages`ã€‚\n",
    "\n",
    "    Args:\n",
    "        prompt (str): ç”¨æˆ·çš„è¾“å…¥æç¤ºè¯ã€‚\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: åŒ…å«è§’è‰²å’Œå†…å®¹çš„æ¶ˆæ¯åˆ—è¡¨ï¼Œç”¨äºæ–‡å¿ƒæ¨¡å‹çš„è¯·æ±‚ã€‚\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \n",
    "                 \"content\": prompt}]\n",
    "    return messages\n",
    "\n",
    "\n",
    "def fetch_wenxin_model_response(prompt: str, \n",
    "                                model: str = \"Yi-34B-Chat\", \n",
    "                                temperature: float = 0.01) -> str:\n",
    "    \"\"\"\n",
    "    ä»æ–‡å¿ƒæ¨¡å‹è·å–å“åº”ã€‚\n",
    "\n",
    "    Args:\n",
    "        prompt (str): è¾“å…¥æç¤ºè¯ã€‚\n",
    "        model (str): è°ƒç”¨çš„æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º \"Yi-34B-Chat\"ã€‚ä¹Ÿå¯ä»¥é€‰æ‹©å…¶ä»–æ¨¡å‹ï¼Œå¦‚ \"ERNIE-Bot-4\"ã€‚\n",
    "        temperature (float): æ¨¡å‹è¾“å‡ºçš„æ¸©åº¦ç³»æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºç¨‹åº¦ã€‚å–å€¼èŒƒå›´ä¸º 0 åˆ° 1.0ï¼Œä¸å¯è®¾ç½®ä¸º 0ã€‚\n",
    "                             æ¸©åº¦ç³»æ•°è¶Šä½ï¼Œè¾“å‡ºå†…å®¹è¶Šä¸€è‡´ã€‚\n",
    "\n",
    "    Returns:\n",
    "        str: æ–‡å¿ƒæ¨¡å‹çš„è¾“å‡ºç»“æœã€‚\n",
    "\n",
    "    Raises:\n",
    "        ValueError: å¦‚æœæ¨¡å‹è°ƒç”¨è¿”å›é”™è¯¯æˆ–ç»“æœä¸ç¬¦åˆé¢„æœŸã€‚\n",
    "    \"\"\"\n",
    "    # å®ä¾‹åŒ–æ–‡å¿ƒæ¨¡å‹çš„ ChatCompletion ç±»\n",
    "    chat_comp = qianfan.ChatCompletion()\n",
    "    # ç”Ÿæˆè¯·æ±‚å‚æ•° messages\n",
    "    message = create_wenxin_request_messages(prompt)\n",
    "\n",
    "    # æ‰§è¡Œæ¨¡å‹è°ƒç”¨\n",
    "    resp = chat_comp.do(messages=message, \n",
    "                        model=model, \n",
    "                        temperature=temperature, \n",
    "                        # system = ,\n",
    "                        penalty_score=1.5)\n",
    "                        # penalty_score \n",
    "                        # å¯¹å·²ç”Ÿæˆçš„tokenå¢åŠ æƒ©ç½šï¼Œå‡å°‘é‡å¤ç”Ÿæˆçš„ç°è±¡ã€‚\n",
    "                        # è¯´æ˜ï¼šï¼ˆ1ï¼‰å€¼è¶Šå¤§è¡¨ç¤ºæƒ©ç½šè¶Šå¤§ã€‚ï¼ˆ2ï¼‰å–å€¼èŒƒå›´ï¼š[1.0, 2.0]ã€‚\n",
    "\n",
    "    # æ£€æŸ¥è¿”å›ç»“æœï¼Œå¦‚æœå­˜åœ¨é”™è¯¯ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸\n",
    "    if resp.get(\"code\") != 200:\n",
    "        raise ValueError(f\"Model call returned an error: {resp.get('msg')}\")\n",
    "\n",
    "    # è¿”å›æ¨¡å‹çš„è¾“å‡ºç»“æœ\n",
    "    return resp[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'æ‚¨å¥½ï¼å¾ˆé«˜å…´è§åˆ°æ‚¨â€”â€”ä½œä¸ºä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œæˆ‘éšæ—¶å‡†å¤‡å¸®åŠ©è§£ç­”é—®é¢˜æˆ–æä¾›ä¿¡æ¯æ”¯æŒã€‚è¯·é—®æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®æ‚¨çš„ï¼Ÿ'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_wenxin_model_response(\"ä½ å¥½ï¼Œæˆ‘æ˜¯æ–‡å¿ƒä¸€è¨€ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¯é£å¹³å° - è®¯é£æ˜Ÿç«\n",
    "# SDKæ–¹å¼è°ƒç”¨\n",
    "# pip install spark-ai-python0.3.15\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "from sparkai.llm.llm import ChatSparkLLM, ChunkPrintHandler\n",
    "from sparkai.core.messages import ChatMessage\n",
    "\n",
    "\n",
    "def build_spark_config(model: str) -> dict:\n",
    "    \"\"\"\n",
    "    æ ¹æ®æŒ‡å®šçš„æ¨¡å‹ç‰ˆæœ¬æ„é€ æ˜Ÿç«æ¨¡å‹çš„è¯·æ±‚é…ç½®ã€‚\n",
    "\n",
    "    Args:\n",
    "        model (str): æ¨¡å‹ç‰ˆæœ¬ï¼Œæ”¯æŒ \"v1.5\"ã€\"v2.0\"ã€\"v3.0\" å’Œ \"v3.5\"ã€‚\n",
    "\n",
    "    Returns:\n",
    "        dict: åŒ…å«æ¨¡å‹ç‰ˆæœ¬å¯¹åº”çš„åŸŸå’ŒAPI URLçš„é…ç½®å­—å…¸ã€‚\n",
    "    \"\"\"\n",
    "    spark_url_template = \"wss://spark-api.xf-yun.com/{}/chat\"\n",
    "    model_configurations = {\n",
    "        \"v1.5\": {\"domain\": \"general\", \"spark_url\": spark_url_template.format(\"v1.1\")},\n",
    "        \"v2.0\": {\"domain\": \"generalv2\", \"spark_url\": spark_url_template.format(\"v2.1\")},\n",
    "        \"v3.0\": {\"domain\": \"generalv3\", \"spark_url\": spark_url_template.format(\"v3.1\")},\n",
    "        \"v3.5\": {\"domain\": \"generalv3.5\", \"spark_url\": spark_url_template.format(\"v3.5\")},\n",
    "    }\n",
    "    return model_configurations[model]\n",
    "\n",
    "def create_spark_request_messages(prompt: str) -> list:\n",
    "    \"\"\"\n",
    "    åˆ›å»ºç”¨äºæ˜Ÿç«æ¨¡å‹è¯·æ±‚çš„æ¶ˆæ¯åˆ—è¡¨ã€‚\n",
    "\n",
    "    Args:\n",
    "        prompt (str): ç”¨æˆ·è¾“å…¥çš„æç¤ºè¯ã€‚\n",
    "\n",
    "    Returns:\n",
    "        list: åŒ…å«å•ä¸ªæ¶ˆæ¯çš„åˆ—è¡¨ï¼Œæ¶ˆæ¯æ ¼å¼ä¸ºå­—å…¸ï¼ŒåŒ…å«ç”¨æˆ·è§’è‰²å’Œå†…å®¹ã€‚\n",
    "    \"\"\"\n",
    "    messages = [ChatMessage(role=\"user\", content=prompt)]\n",
    "    return messages\n",
    "\n",
    "def fetch_spark_model_response(prompt: str, \n",
    "                               model: str = \"v3.5\", \n",
    "                               temperature: float = 0.1) -> str:\n",
    "    \"\"\"\n",
    "    è°ƒç”¨æ˜Ÿç«æ¨¡å‹å¹¶è·å–è¾“å‡ºç»“æœã€‚\n",
    "\n",
    "    Args:\n",
    "        prompt (str): è¾“å…¥çš„æç¤ºè¯ã€‚\n",
    "        model (str): è¦è°ƒç”¨çš„æ¨¡å‹ç‰ˆæœ¬ï¼Œé»˜è®¤ä¸º \"v3.5\"ï¼Œä¹Ÿæ”¯æŒ \"v3.0\" ç­‰å…¶ä»–ç‰ˆæœ¬ã€‚\n",
    "        temperature (float): æ¨¡å‹è¾“å‡ºçš„æ¸©åº¦ç³»æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºç¨‹åº¦ï¼Œå–å€¼èŒƒå›´ä¸º 0~1.0ï¼Œä¸èƒ½ä¸º 0ã€‚\n",
    "\n",
    "    Returns:\n",
    "        str: æ˜Ÿç«æ¨¡å‹çš„è°ƒç”¨ç»“æœã€‚\n",
    "    \"\"\"\n",
    "    spark_configuration = build_spark_config(model)\n",
    "    \n",
    "    spark_llm = ChatSparkLLM(\n",
    "        spark_api_url=spark_configuration[\"spark_url\"],\n",
    "        spark_app_id=os.environ[\"SPARK_APPID\"],\n",
    "        spark_api_key=os.environ[\"SPARK_API_KEY\"],\n",
    "        spark_api_secret=os.environ[\"SPARK_API_SECRET\"],\n",
    "        spark_llm_domain=spark_configuration[\"domain\"],\n",
    "        temperature=temperature,\n",
    "        streaming=False,\n",
    "    )\n",
    "    messages = create_spark_request_messages(prompt)\n",
    "    handler = ChunkPrintHandler()\n",
    "    response = spark_llm.generate([messages], callbacks=[handler])\n",
    "    return response.generations[0][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘èƒ½å¸®å¿™çš„å—ï¼Ÿ'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_spark_model_response(\"ä½ å¥½\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<zhipuai._client.ZhipuAI object at 0x7faaf269c0d0>\n"
     ]
    }
   ],
   "source": [
    "# æ™ºè°±GLM \n",
    "# %pip install zhipuai==2.0.1\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from zhipuai import ZhipuAI\n",
    "_ = load_dotenv(find_dotenv())\n",
    "client = ZhipuAI(api_key=os.getenv(\"ZHIPU_API_KEY\"))\n",
    "print(client)\n",
    "\n",
    "def create_zhipuai_request_messages(prompt: str) -> list:\n",
    "    \"\"\"\n",
    "    åˆ›å»ºç”¨äºæ™ºè°±GLMæ¨¡å‹è¯·æ±‚çš„æ¶ˆæ¯åˆ—è¡¨ã€‚\n",
    "\n",
    "    Args:\n",
    "        prompt (str): ç”¨æˆ·è¾“å…¥çš„æç¤ºè¯ã€‚\n",
    "\n",
    "    Returns:\n",
    "        list: åŒ…å«å•ä¸ªæ¶ˆæ¯çš„åˆ—è¡¨ï¼Œæ¶ˆæ¯æ ¼å¼ä¸ºå­—å…¸ï¼ŒåŒ…å«ç”¨æˆ·è§’è‰²å’Œå†…å®¹ã€‚\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return messages\n",
    "\n",
    "\n",
    "def fetch_zhipuai_model_response(prompt: str, \n",
    "                                 model: str = \"glm-4\", \n",
    "                                 temperature: float = 0.1) -> str:\n",
    "    \"\"\"\n",
    "    è°ƒç”¨æ™ºè°±GLMæ¨¡å‹å¹¶è·å–è¾“å‡ºç»“æœã€‚\n",
    "\n",
    "    Args:\n",
    "        prompt (str): è¾“å…¥çš„æç¤ºè¯ã€‚\n",
    "        model (str): è¦è°ƒç”¨çš„æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º \"zhipuai-glm\"ã€‚\n",
    "        temperature (float): æ¨¡å‹è¾“å‡ºçš„æ¸©åº¦ç³»æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºç¨‹åº¦ï¼Œå–å€¼èŒƒå›´ä¸º 0~1.0ï¼Œä¸èƒ½ä¸º 0ã€‚\n",
    "\n",
    "    Returns:\n",
    "        str: æ™ºè°±GLMæ¨¡å‹çš„è°ƒç”¨ç»“æœã€‚\n",
    "    \"\"\"\n",
    "    messages = create_zhipuai_request_messages(prompt)\n",
    "    response = client.chat.completions.create(model=model,\n",
    "                                              messages=messages,\n",
    "                                              temperature=temperature)\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹æ™ºè°±æ¸…è¨€ï¼Œå¯ä»¥å«æˆ‘å°æ™ºğŸ¤–ï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_zhipuai_model_response(\"ä½ å¥½\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MChat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
