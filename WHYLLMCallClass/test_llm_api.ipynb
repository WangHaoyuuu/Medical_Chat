{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 包含三种大模型api调用的测试\n",
    "\n",
    "# 同一的函数调用\n",
    "# create_modelname_request_message()\n",
    "# fetch__modelname_model_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 千帆平台 - 文心一言\n",
    "# %pip install qianfan==0.3.9\n",
    "import qianfan\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from typing import Dict, List\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "def create_wenxin_request_messages(prompt: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    构造用于文心模型的请求参数 `messages`。\n",
    "\n",
    "    Args:\n",
    "        prompt (str): 用户的输入提示词。\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: 包含角色和内容的消息列表，用于文心模型的请求。\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \n",
    "                 \"content\": prompt}]\n",
    "    return messages\n",
    "\n",
    "\n",
    "def fetch_wenxin_model_response(prompt: str, \n",
    "                                model: str = \"Yi-34B-Chat\", \n",
    "                                temperature: float = 0.01) -> str:\n",
    "    \"\"\"\n",
    "    从文心模型获取响应。\n",
    "\n",
    "    Args:\n",
    "        prompt (str): 输入提示词。\n",
    "        model (str): 调用的模型名称，默认为 \"Yi-34B-Chat\"。也可以选择其他模型，如 \"ERNIE-Bot-4\"。\n",
    "        temperature (float): 模型输出的温度系数，控制输出的随机程度。取值范围为 0 到 1.0，不可设置为 0。\n",
    "                             温度系数越低，输出内容越一致。\n",
    "\n",
    "    Returns:\n",
    "        str: 文心模型的输出结果。\n",
    "\n",
    "    Raises:\n",
    "        ValueError: 如果模型调用返回错误或结果不符合预期。\n",
    "    \"\"\"\n",
    "    # 实例化文心模型的 ChatCompletion 类\n",
    "    chat_comp = qianfan.ChatCompletion()\n",
    "    # 生成请求参数 messages\n",
    "    message = create_wenxin_request_messages(prompt)\n",
    "\n",
    "    # 执行模型调用\n",
    "    resp = chat_comp.do(messages=message, \n",
    "                        model=model, \n",
    "                        temperature=temperature, \n",
    "                        # system = ,\n",
    "                        penalty_score=1.5)\n",
    "                        # penalty_score \n",
    "                        # 对已生成的token增加惩罚，减少重复生成的现象。\n",
    "                        # 说明：（1）值越大表示惩罚越大。（2）取值范围：[1.0, 2.0]。\n",
    "\n",
    "    # 检查返回结果，如果存在错误，则抛出异常\n",
    "    if resp.get(\"code\") != 200:\n",
    "        raise ValueError(f\"Model call returned an error: {resp.get('msg')}\")\n",
    "\n",
    "    # 返回模型的输出结果\n",
    "    return resp[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'您好！很高兴见到您——作为一个人工智能助手，我随时准备帮助解答问题或提供信息支持。请问有什么我可以帮您的？'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_wenxin_model_response(\"你好，我是文心一言。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讯飞平台 - 讯飞星火\n",
    "# SDK方式调用\n",
    "# pip install spark-ai-python0.3.15\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "from sparkai.llm.llm import ChatSparkLLM, ChunkPrintHandler\n",
    "from sparkai.core.messages import ChatMessage\n",
    "\n",
    "\n",
    "def build_spark_config(model: str) -> dict:\n",
    "    \"\"\"\n",
    "    根据指定的模型版本构造星火模型的请求配置。\n",
    "\n",
    "    Args:\n",
    "        model (str): 模型版本，支持 \"v1.5\"、\"v2.0\"、\"v3.0\" 和 \"v3.5\"。\n",
    "\n",
    "    Returns:\n",
    "        dict: 包含模型版本对应的域和API URL的配置字典。\n",
    "    \"\"\"\n",
    "    spark_url_template = \"wss://spark-api.xf-yun.com/{}/chat\"\n",
    "    model_configurations = {\n",
    "        \"v1.5\": {\"domain\": \"general\", \"spark_url\": spark_url_template.format(\"v1.1\")},\n",
    "        \"v2.0\": {\"domain\": \"generalv2\", \"spark_url\": spark_url_template.format(\"v2.1\")},\n",
    "        \"v3.0\": {\"domain\": \"generalv3\", \"spark_url\": spark_url_template.format(\"v3.1\")},\n",
    "        \"v3.5\": {\"domain\": \"generalv3.5\", \"spark_url\": spark_url_template.format(\"v3.5\")},\n",
    "    }\n",
    "    return model_configurations[model]\n",
    "\n",
    "def create_spark_request_messages(prompt: str) -> list:\n",
    "    \"\"\"\n",
    "    创建用于星火模型请求的消息列表。\n",
    "\n",
    "    Args:\n",
    "        prompt (str): 用户输入的提示词。\n",
    "\n",
    "    Returns:\n",
    "        list: 包含单个消息的列表，消息格式为字典，包含用户角色和内容。\n",
    "    \"\"\"\n",
    "    messages = [ChatMessage(role=\"user\", content=prompt)]\n",
    "    return messages\n",
    "\n",
    "def fetch_spark_model_response(prompt: str, \n",
    "                               model: str = \"v3.5\", \n",
    "                               temperature: float = 0.1) -> str:\n",
    "    \"\"\"\n",
    "    调用星火模型并获取输出结果。\n",
    "\n",
    "    Args:\n",
    "        prompt (str): 输入的提示词。\n",
    "        model (str): 要调用的模型版本，默认为 \"v3.5\"，也支持 \"v3.0\" 等其他版本。\n",
    "        temperature (float): 模型输出的温度系数，控制输出的随机程度，取值范围为 0~1.0，不能为 0。\n",
    "\n",
    "    Returns:\n",
    "        str: 星火模型的调用结果。\n",
    "    \"\"\"\n",
    "    spark_configuration = build_spark_config(model)\n",
    "    \n",
    "    spark_llm = ChatSparkLLM(\n",
    "        spark_api_url=spark_configuration[\"spark_url\"],\n",
    "        spark_app_id=os.environ[\"SPARK_APPID\"],\n",
    "        spark_api_key=os.environ[\"SPARK_API_KEY\"],\n",
    "        spark_api_secret=os.environ[\"SPARK_API_SECRET\"],\n",
    "        spark_llm_domain=spark_configuration[\"domain\"],\n",
    "        temperature=temperature,\n",
    "        streaming=False,\n",
    "    )\n",
    "    messages = create_spark_request_messages(prompt)\n",
    "    handler = ChunkPrintHandler()\n",
    "    response = spark_llm.generate([messages], callbacks=[handler])\n",
    "    return response.generations[0][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好！有什么我能帮忙的吗？'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_spark_model_response(\"你好\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<zhipuai._client.ZhipuAI object at 0x7faaf269c0d0>\n"
     ]
    }
   ],
   "source": [
    "# 智谱GLM \n",
    "# %pip install zhipuai==2.0.1\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from zhipuai import ZhipuAI\n",
    "_ = load_dotenv(find_dotenv())\n",
    "client = ZhipuAI(api_key=os.getenv(\"ZHIPU_API_KEY\"))\n",
    "print(client)\n",
    "\n",
    "def create_zhipuai_request_messages(prompt: str) -> list:\n",
    "    \"\"\"\n",
    "    创建用于智谱GLM模型请求的消息列表。\n",
    "\n",
    "    Args:\n",
    "        prompt (str): 用户输入的提示词。\n",
    "\n",
    "    Returns:\n",
    "        list: 包含单个消息的列表，消息格式为字典，包含用户角色和内容。\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return messages\n",
    "\n",
    "\n",
    "def fetch_zhipuai_model_response(prompt: str, \n",
    "                                 model: str = \"glm-4\", \n",
    "                                 temperature: float = 0.1) -> str:\n",
    "    \"\"\"\n",
    "    调用智谱GLM模型并获取输出结果。\n",
    "\n",
    "    Args:\n",
    "        prompt (str): 输入的提示词。\n",
    "        model (str): 要调用的模型名称，默认为 \"zhipuai-glm\"。\n",
    "        temperature (float): 模型输出的温度系数，控制输出的随机程度，取值范围为 0~1.0，不能为 0。\n",
    "\n",
    "    Returns:\n",
    "        str: 智谱GLM模型的调用结果。\n",
    "    \"\"\"\n",
    "    messages = create_zhipuai_request_messages(prompt)\n",
    "    response = client.chat.completions.create(model=model,\n",
    "                                              messages=messages,\n",
    "                                              temperature=temperature)\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好👋！我是人工智能助手智谱清言，可以叫我小智🤖，很高兴见到你，欢迎问我任何问题。'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_zhipuai_model_response(\"你好\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MChat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
