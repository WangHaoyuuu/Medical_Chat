{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 包含智谱的embedding 和 其他开源的embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 智谱测试成功\n",
    "import os\n",
    "from zhipuai import ZhipuAI\n",
    "def zhipu_embedding(text: str):\n",
    "\n",
    "    api_key = os.environ['ZHIPUAI_API_KEY']\n",
    "    client = ZhipuAI(api_key=api_key)\n",
    "    response = client.embeddings.create(\n",
    "        model=\"embedding-2\",\n",
    "        input=text,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "text = '要生成 embedding 的输入文本，字符串形式。'\n",
    "response = zhipu_embedding(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response类型为：<class 'zhipuai.types.embeddings.EmbeddingsResponded'>\n",
      "embedding类型为：list\n",
      "生成embedding的model为：embedding-2\n",
      "生成的embedding长度为：1024\n",
      "embedding（前10）为: [0.01789241097867489, 0.0644201934337616, -0.009342828765511513, 0.027074744924902916, 0.004067827481776476, -0.055978596210479736, -0.042238060384988785, -0.030031980946660042, -0.016357745975255966, 0.06777039915323257]\n"
     ]
    }
   ],
   "source": [
    "print(f'response类型为：{type(response)}')\n",
    "print(f'embedding类型为：{response.object}')\n",
    "print(f'生成embedding的model为：{response.model}')\n",
    "print(f'生成的embedding长度为：{len(response.data[0].embedding)}')\n",
    "print(f'embedding（前10）为: {response.data[0].embedding[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "载入后的变量类型为：<class 'list'>， 该 Markdown 一共包含 1 页\n"
     ]
    }
   ],
   "source": [
    "# huggingface 上的开源embedding模型\n",
    "# moka-ai/m3e-base\n",
    "# 读出markdown文件\n",
    "from langchain.document_loaders.markdown import UnstructuredMarkdownLoader\n",
    "\n",
    "loader = UnstructuredMarkdownLoader(\"/workspaces/Medical_Chat/knowledge_db/test.md\")\n",
    "md_pages = loader.load()\n",
    "print(f\"载入后的变量类型为：{type(md_pages)}，\",  f\"该 Markdown 一共包含 {len(md_pages)} 页\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每一个元素的类型：<class 'langchain_core.documents.base.Document'>.\n",
      "------\n",
      "该文档的描述性数据：{'source': '/workspaces/Medical_Chat/knowledge_db/test.md'}\n",
      "------\n",
      "查看该文档的内容:\n",
      "恭喜您完成了本书第一单元内容的学习！\n",
      "\n",
      "总的来说，在第一部分中，我们学习并掌握了关于 Prompt 的两个核心原则：\n",
      "\n",
      "编写清晰具体的指令；\n",
      "如果适当的话，给模型一些思考时间。\n",
      "您还学习了迭代式 Prompt 开发的方法，并了解了如何找到适合您应用程序的 Prompt 的过程是非常关键的。\n",
      "\n",
      "我们还讨论了大型语言模型的许多功能，包括摘要、推断、转换和扩展。您也学习了如何搭建个性化的聊天机器人。在第\n"
     ]
    }
   ],
   "source": [
    "md_page = md_pages[0]\n",
    "print(f\"每一个元素的类型：{type(md_page)}.\", \n",
    "    f\"该文档的描述性数据：{md_page.metadata}\", \n",
    "    f\"查看该文档的内容:\\n{md_page.page_content[0:][:200]}\", \n",
    "    sep=\"\\n------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "恭喜您完成了本书第一单元内容的学习！总的来说，在第一部分中，我们学习并掌握了关于 Prompt 的两个核心原则：编写清晰具体的指令；\n",
      "如果适当的话，给模型一些思考时间。\n",
      "您还学习了迭代式 Prompt 开发的方法，并了解了如何找到适合您应用程序的 Prompt 的过程是非常关键的。我们还讨论了大型语言模型的许多功能，包括摘要、推断、转换和扩展。您也学习了如何搭建个性化的聊天机器人。在第一部分中，您的收获应该颇丰，希望通过第一部分学习能为您带来愉悦的体验。我们期待您能灵感迸发，尝试创建自己的应用。请大胆尝试，并分享给我们您的想法。您可以从一个微型项目开始，或许它具备一定的实用性，或者仅仅是一项有趣的创新。请利用您在第一个项目中得到的经验，去创造更优秀的下一项目，以此类推。如果您已经有一个宏大的项目设想，那么，请毫不犹豫地去实现它。最后，希望您在完成第一部分的过程中感到满足，感谢您的参与。我们热切期待着您的惊艳作品。接下来，我们将进入第二部分的学习！\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = re.compile(r'[^\\u4e00-\\u9fff](\\n)[^\\u4e00-\\u9fff]', re.DOTALL)\n",
    "for page in md_pages:\n",
    "    page.page_content = re.sub(pattern, lambda match: match.group(0).replace('\\n', ''), page.page_content)\n",
    "    print(page.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "载入后的变量类型为：<class 'list'>， 该 Markdown 一共包含 1 页\n",
      "每一个元素的类型：<class 'langchain_core.documents.base.Document'>.\n",
      "------\n",
      "该文档的描述性数据：{'source': '/workspaces/Medical_Chat/knowledge_db/test.md'}\n",
      "------\n",
      "查看该文档的内容:\n",
      "恭喜您完成了本书第一单元内容的学习！\n",
      "\n",
      "总的来说，在第一部分中，我们学习并掌握了关于 Prompt 的两个核心原则：\n",
      "\n",
      "编写清晰具体的指令；\n",
      "如果适当的话，给模型一些思考时间。\n",
      "您还学习了迭代式 Prompt 开发的方法，并了解了如何找到适合您应用程序的 Prompt 的过程是非常关键的。\n",
      "\n",
      "我们还讨论了大型语言模型的许多功能，包括摘要、推断、转换和扩展。您也学习了如何搭建个性化的聊天机器人。在第\n",
      "恭喜您完成了本书第一单元内容的学习！总的来说，在第一部分中，我们学习并掌握了关于 Prompt 的两个核心原则：编写清晰具体的指令；\n",
      "如果适当的话，给模型一些思考时间。\n",
      "您还学习了迭代式 Prompt 开发的方法，并了解了如何找到适合您应用程序的 Prompt 的过程是非常关键的。我们还讨论了大型语言模型的许多功能，包括摘要、推断、转换和扩展。您也学习了如何搭建个性化的聊天机器人。在第一部分中，您的收获应该颇丰，希望通过第一部分学习能为您带来愉悦的体验。我们期待您能灵感迸发，尝试创建自己的应用。请大胆尝试，并分享给我们您的想法。您可以从一个微型项目开始，或许它具备一定的实用性，或者仅仅是一项有趣的创新。请利用您在第一个项目中得到的经验，去创造更优秀的下一项目，以此类推。如果您已经有一个宏大的项目设想，那么，请毫不犹豫地去实现它。最后，希望您在完成第一部分的过程中感到满足，感谢您的参与。我们热切期待着您的惊艳作品。接下来，我们将进入第二部分的学习！\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.markdown import UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "# from langchain.document_loaders.markdown import UnstructuredMarkdownLoader, Document\n",
    "import re\n",
    "\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, page_content, metadata=None):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata\n",
    "\n",
    "\n",
    "loader = UnstructuredMarkdownLoader(\"/workspaces/Medical_Chat/knowledge_db/test.md\")\n",
    "md_pages = loader.load()\n",
    "\n",
    "print(f\"载入后的变量类型为：{type(md_pages)}，\",  f\"该 Markdown 一共包含 {len(md_pages)} 页\")\n",
    "md_page = md_pages[0]\n",
    "print(f\"每一个元素的类型：{type(md_page)}.\", \n",
    "    f\"该文档的描述性数据：{md_page.metadata}\", \n",
    "    f\"查看该文档的内容:\\n{md_page.page_content[0:][:200]}\", \n",
    "    sep=\"\\n------\\n\")\n",
    "\n",
    "pattern = re.compile(r'[^\\u4e00-\\u9fff](\\n)[^\\u4e00-\\u9fff]', re.DOTALL)\n",
    "for page in md_pages:\n",
    "    page.page_content = re.sub(pattern, lambda match: match.group(0).replace('\\n', ''), page.page_content)\n",
    "    print(page.page_content)\n",
    "\n",
    "CHUNK_SIZE = 20\n",
    "OVERLAP_SIZE = 5\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=OVERLAP_SIZE\n",
    ")\n",
    "\n",
    "split_docs = [Document(page_content=doc, metadata=md_page.metadata) for doc in text_splitter.split_text(md_page.page_content)]\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"moka-ai/m3e-base\")\n",
    "persist_directory = 'WHYembedding/vector_db'\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的内容数：3\n",
      "检索到的第0个内容: \n",
      "搭建个性化的聊天机器\n",
      "--------------\n",
      "检索到的第1个内容: \n",
      "学习了如何搭建个性化的聊天机器人。在第一\n",
      "--------------\n",
      "检索到的第2个内容: \n",
      "的聊天机器人。在第一\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "# print(f\"向量库中存储的数量：{vectordb._collection.count()}\")\n",
    "question = \"聊天\"\n",
    "sim_docs = vectordb.similarity_search(question,k=3)\n",
    "print(f\"检索到的内容数：{len(sim_docs)}\")\n",
    "# print(sim_docs[0])\n",
    "for i, sim_doc in enumerate(sim_docs):\n",
    "    print(f\"检索到的第{i}个内容: \\n{sim_doc.page_content[:200]}\", end=\"\\n--------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.markdown import UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "import re\n",
    "\n",
    "loader = UnstructuredMarkdownLoader(\"/workspaces/Medical_Chat/knowledge_db/test.md\")\n",
    "md_pages = loader.load()\n",
    "\n",
    "print(f\"载入后的变量类型为：{type(md_pages)}，\",  f\"该 Markdown 一共包含 {len(md_pages)} 页\")\n",
    "md_page = md_pages[0]\n",
    "print(f\"每一个元素的类型：{type(md_page)}.\", \n",
    "    f\"该文档的描述性数据：{md_page.metadata}\", \n",
    "    f\"查看该文档的内容:\\n{md_page.page_content[0:][:200]}\", \n",
    "    sep=\"\\n------\\n\")\n",
    "\n",
    "pattern = re.compile(r'[^\\u4e00-\\u9fff](\\n)[^\\u4e00-\\u9fff]', re.DOTALL)\n",
    "for page in md_pages:\n",
    "    page.page_content = re.sub(pattern, lambda match: match.group(0).replace('\\n', ''), page.page_content)\n",
    "    print(page.page_content)\n",
    "\n",
    "CHUNK_SIZE = 500\n",
    "OVERLAP_SIZE = 50\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=OVERLAP_SIZE\n",
    ")\n",
    "\n",
    "split_docs = [UnstructuredMarkdownLoader(page_content=doc) for doc in text_splitter.split_text(md_page.page_content)]\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"moka-ai/m3e-base\")\n",
    "persist_directory = 'WHYembedding/vector_db'\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 不同科室的数据放在不同的数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "from embedding.call_embedding import get_embedding\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "# 首先我只处理pdf文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共找到2个PDF文件\n",
      "正在处理文件：/workspaces/Medical_Chat/WHYmedicalbooks/临床科/临床流行病学.pdf\n",
      "持久化路径为：/workspaces/Medical_Chat/WHYembedding/临床科/vector_db\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/workspaces/Medical_Chat/WHYembedding/临床科'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m pdf_files \u001b[38;5;241m=\u001b[39m get_files(DEFAULT_DB_PATH)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m共找到\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pdf_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m个PDF文件\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m \u001b[43mcreate_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpdf_files\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 51\u001b[0m, in \u001b[0;36mcreate_db\u001b[0;34m(files)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m持久化路径为：\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpersist_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(persist_dir):\n\u001b[0;32m---> 51\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpersist_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 创建持久化目录\u001b[39;00m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;66;03m# Check if the directory exists and has write permissions\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39maccess(persist_dir, os\u001b[38;5;241m.\u001b[39mW_OK):\n",
      "File \u001b[0;32m/opt/conda/envs/MChat/lib/python3.9/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/MChat/lib/python3.9/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/workspaces/Medical_Chat/WHYembedding/临床科'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import errno\n",
    "import os\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "DEFAULT_DB_PATH = \"/workspaces/Medical_Chat/WHYmedicalbooks\"\n",
    "DEFAULT_PERSIST_PATH = \"/workspaces/Medical_Chat/WHYembedding\"\n",
    "\n",
    "def get_files(dir_path):\n",
    "    file_list = []\n",
    "    for filepath, dirnames, filenames in os.walk(dir_path):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.pdf'):  # 确保只处理PDF文件\n",
    "                file_list.append(os.path.join(filepath, filename))\n",
    "    return file_list\n",
    "\n",
    "def create_db(files):\n",
    "    if files is None:\n",
    "        return \"can't load empty file\"\n",
    "    if not isinstance(files, list):\n",
    "        files = [files]\n",
    "\n",
    "    for file_path in files:\n",
    "        print(f\"正在处理文件：{file_path}\")\n",
    "        loader = PyMuPDFLoader(file_path)\n",
    "        docs = loader.load()  # 假设这会返回一个包含文档内容的列表\n",
    "        \n",
    "        # 切分文档\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=150)\n",
    "        split_docs = text_splitter.split_documents(docs)\n",
    "        \n",
    "        # 获取嵌入向量\n",
    "        embedding = HuggingFaceEmbeddings(model_name=\"moka-ai/m3e-base\")\n",
    "        \n",
    "        # 为每个文件的父目录创建唯一的持久化路径\n",
    "        parent_dir_name = Path(file_path).parent.stem\n",
    "\n",
    "        persist_dir = os.path.join(DEFAULT_PERSIST_PATH, parent_dir_name, \"vector_db\")\n",
    "        \n",
    "        print(f\"持久化路径为：{persist_dir}\")\n",
    "        if not os.path.exists(persist_dir):\n",
    "            print\n",
    "            os.makedirs(persist_dir, exist_ok=True)  # 创建持久化目录\n",
    "        \n",
    "\n",
    "                # Check if the directory exists and has write permissions\n",
    "        if not os.access(persist_dir, os.W_OK):\n",
    "            try:\n",
    "                os.makedirs(persist_dir, exist_ok=True)  # Try to create the directory\n",
    "            except OSError as e:\n",
    "                if e.errno != errno.EEXIST:  # Ignore error if directory already exists\n",
    "                    raise\n",
    "                print(f\"Error: The directory {persist_dir} cannot be written to.\")\n",
    "                continue\n",
    "            \n",
    "        # 创建向量数据库\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=split_docs[:10],\n",
    "            embedding=embedding,\n",
    "            persist_directory=persist_dir\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 获取所有子目录中的PDF文件\n",
    "    pdf_files = get_files(DEFAULT_DB_PATH)\n",
    "    print(f\"共找到{len(pdf_files)}个PDF文件\")\n",
    "    create_db(files=pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_community.vectorstores.chroma.Chroma'>\n",
      "向量库中存储的数量：10\n",
      "检索到的内容数：3\n",
      "检索到的第0个内容: \n",
      " 目录21一、流行病学233二、病因233三、临床表现234四、病程与预后 236五、诊断与鉴别诊断236六、预防与治疗237第三节孤独症谱系障碍239一、概述239二、流行病学239三、临床表现239四、诊断与鉴别诊断241五、病程及预后 242六、干预和治疗242第四节注意缺陷多动性障碍243一、流行病学244二、病因和病理机制244三、临床表现244四、诊断与鉴别诊断245五、病程和预后246六、治疗和干预247第五节抽动障碍 249一、流行病学 249二、病因和病理机制249三、临床表现249四、诊断与鉴别诊断 250五、病程与预后251六、治疗 251第六节特定性学习障碍252第十三章精神分裂症 256第一节概述 256第二节病因与发病机制257一、遗传因素257二、神经病理学及大脑结构的异常 259三、神经生化方面的异常261四、神经发育不良假说261五、子宫内感染与产伤 262六、社会心理因素263第三节临床表现263第四节诊断与鉴别诊断 264一、诊断标准264二、鉴别诊断 266三、实验室检查 266\n",
      "***************************************************\n",
      "检索到的第1个内容: \n",
      " 24■目录四、病程和预后 331五、病因和发病机制研究331六、治疗331七、预防和康复 331第四节社交焦虑障碍331一、流行病学 332二、临床表现 332三、诊断与鉴别诊断 332四、病程和预后333五、病因和发病机制研究 333六、治疗 333七、预防和康复334第五节特定恐惧症334一、流行病学 335二、临床表现 335三、诊断与鉴别诊断 335四、病程和预后335五、病因和发病机制研究 336六、治疗 336七、预防和康复336第六节广泛性焦虑障碍337一、流行病学 337二、临床表现 337三、诊断与鉴别诊断337四、病程和预后338五、病因和发病机制研究339六、治疗 339七、预防和康复340第七节分离焦虑障碍340一、流行病学 340二、临床表现341三、诊断与鉴别诊断 341四、病程和预后 341五、病因和发病机制研究 342六、治疗 342七、预防和康复 342第十八章强迫症 344第一节概述 344第二节病因与发病机制345一、生物学因素 345二、社会心理因素346第三节临床表现 348\n",
      "***************************************************\n",
      "检索到的第2个内容: \n",
      " 193一、概念193二、病理生理机制 194三、临床表现的共同特点194四、常见临床表现 194五、诊断 194六、治疗原则194第二节躯体感染所致精神障碍195一、流行性感冒所致精种障碍 195二、肺炎所致精神障碍195三、伤寒所致精神障碍 196四、病毒性肝炎所致精神障碍 196五、治疗原则196第三节内脏器官疾病所致精神障碍196一、肺脑综合征的临床表现196二、心脏疾病所致精神障碍的临床表现197三、肝脏疾病所致精神障碍的临床表现197四、肾脏疾病所致精神障碍的临床表现198五、内脏器官疾病所致精神障碍的诊断 198六、内脏器官疾病所致精神障碍治疗 198\n",
      "***************************************************\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# 加载向量知识库\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"moka-ai/m3e-base\")\n",
    "persist_directory = '/workspaces/Medical_Chat/WHYembedding/精神科/vector_db'\n",
    "# 加载数据库\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,  # 允许我们将persist_directory目录保存到磁盘上\n",
    "    embedding_function=embedding\n",
    ")\n",
    "print(type(vectordb))\n",
    "print(f\"向量库中存储的数量：{vectordb._collection.count()}\")\n",
    "question = \"临床?\"\n",
    "docs = vectordb.similarity_search(question,k=3)\n",
    "print(f\"检索到的内容数：{len(docs)}\")\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"检索到的第{i}个内容: \\n {doc.page_content}\", end=\"\\n***************************************************\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MChat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
