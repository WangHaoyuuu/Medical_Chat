{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ['常见导致营养不良的原因', \n",
    "        '根据提供的信息，常见导致营养不良的原因包括：1. 摄入食物的减少：这可能是因为食物供应不足、经济条件限制、不良的饮食习惯或食物选择有限等原因。2. 营养素吸收障碍和丢失：这可能与消化系统疾病、肠道菌群失衡、营养素吸收不良等因素有关。3. 厌食：这可能由多种因素引起，包括心理因素（如情绪压力、焦虑或抑郁）、生理因素（如激素水平变化、新陈代谢异常）、疾病因素（如胃肠炎、肝功能异常）等。4. 吞咽机制障碍疼痛导致的厌食症：这可能与口腔或咽喉部的疼痛、炎症、损伤或其他疾病有关。5. 梗阻致使的呕吐：这可能是因为消化道梗阻，导致食物和营养素无法正常吸收和利用。6. 遗传代谢性疾病：这可能是一组遗传性代谢异常疾病，导致身体无法正常利用或合成某些必需的营养素。7. 其他原因：如胆道闭锁、先天性心脏病、染色体异常、Alagille综合征等，这些疾病可能伴随营养不良的症状。请注意，上述信息是基于提供的文本内容，如果需要更详细或最新的信息，建议咨询专业的医疗或营养专家。']\n",
    "# 将'常见导致营养不良的原因', '根据提供的信息，常见导致营养不良的原因包括：1. 摄入食物的减少：这可能是因为食物供应不足、经济条件限制、不良的饮食习惯或食物选择有限等原因。2. 营养素吸收障碍和丢失：这可能与消化系统疾病、肠道菌群失衡、营养素吸收不良等因素有关。3. 厌食：这可能由多种因素引起，包括心理因素（如情绪压力、焦虑或抑郁）、生理因素（如激素水平变化、新陈代谢异常）、疾病因素（如胃肠炎、肝功能异常）等。4. 吞咽机制障碍疼痛导致的厌食症：这可能与口腔或咽喉部的疼痛、炎症、损伤或其他疾病有关。5. 梗阻致使的呕吐：这可能是因为消化道梗阻，导致食物和营养素无法正常吸收和利用。6. 遗传代谢性疾病：这可能是一组遗传性代谢异常疾病，导致身体无法正常利用或合成某些必需的营养素。7. 其他原因：如胆道闭锁、先天性心脏病、染色体异常、Alagille综合征等，这些疾病可能伴随营养不良的症状。请注意，上述信息是基于提供的文本内容，如果需要更详细或最新的信息，建议咨询专业的医疗或营养专家。'转换成元组\n",
    "print(test())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import sys \n",
    "import os\n",
    "import os               \n",
    "import os\n",
    "import ipykernel\n",
    "import pathlib\n",
    "\n",
    "# 获取当前 notebook 的路径\n",
    "notebook_path = os.path.join(os.path.dirname(ipykernel.get_connection_file()), 'test_run_gradio.ipynb')\n",
    "\n",
    "# 获取当前脚本的绝对路径\n",
    "script_dir = pathlib.Path(notebook_path).parent.absolute()\n",
    "\n",
    "# print(script_dir)\n",
    "\n",
    "# 构造模块的相对路径\n",
    "module_dirs = [\"../\", \"../WHYLLMCallClass\", \"../WHYQA\"]\n",
    "# 添加模块的路径到 sys.path\n",
    "sys.path.extend([os.path.abspath(os.path.join(script_dir, module_dir)) for module_dir in module_dirs])\n",
    "\n",
    "#print(sys.path)\n",
    "\n",
    "from WHYQA.model_to_llm import get_completion\n",
    "from WHYQA.QA_chain_self import QA_chain_self\n",
    "from WHYQA.Chat_QA_chain_self import Chat_QA_chain_self\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"moka-ai/m3e-base\")\n",
    "\n",
    "# print(get_completion(model=\"Yi-34B-Chat\", input=\"你好\"))\n",
    "# qa = QA_chain_self(model=\"Yi-34B-Chat\",embedding=embedding)\n",
    "# print(qa.answer(\"什么是精神科疾病?\"))\n",
    "# qa = Chat_QA_chain_self(model=\"Yi-34B-Chat\",embedding=embedding)\n",
    "# print(qa.answer(\"什么是精神疾病?\"))\n",
    "\n",
    "# LLM模型选择\n",
    "LLM_MODEL_DICT = {\n",
    "    \"QIANFAN\":[\"Yi-34B-Chat\"],\n",
    "    \"SPARK\":[\"Sparkv3\"],\n",
    "    \"ZHIPU\":[\"glm-3-turbo\"]\n",
    "}\n",
    "\n",
    "LLM_MODEL_LIST = sum(list(LLM_MODEL_DICT.values()),[])\n",
    "INIT_MODEL = LLM_MODEL_LIST[0]\n",
    "# print(INIT_MODEL)\n",
    "# print(LLM_MODEL_LIST)\n",
    "\n",
    "Ddfault_PERSIST_PATH = '/workspaces/Medical_Chat/WHYembedding/精神科/vector_db'\n",
    "\n",
    "# 暂时未用到\n",
    "def get_model_by_platform(platform):\n",
    "    return LLM_MODEL_DICT.get(platform,\"\")\n",
    "\n",
    "# print(get_model_by_platform(\"QIANFAN\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class qa_chain():\n",
    "    def __init__(self):\n",
    "        self.chat_qa_chain_self = {}\n",
    "        self.qa_chain_self = {}\n",
    "\n",
    "    def chat_qa_chain_self_answer(self,\n",
    "                                 model:str, \n",
    "                                 embedding = embedding,\n",
    "                                 temperature:float=0.36, \n",
    "                                 question:str=None,\n",
    "                                 appid:str=None,\n",
    "                                 api_key:str=None,\n",
    "                                 api_secret:str=None,\n",
    "                                 persist_path:str='/workspaces/Medical_Chat/WHYembedding/精神科/vector_db',\n",
    "                                 top_k:int=3,\n",
    "                                 chat_history:list=[],):\n",
    "        \"\"\"\n",
    "            chat_qa_chain_self_answer\n",
    "            调用有历史链的回答\n",
    "        \"\"\"\n",
    "        if question == None or len(question) == 0:\n",
    "            return \"请输入问题\", chat_history\n",
    "        try:\n",
    "            key = (model, id(embedding))\n",
    "            if key not in self.chat_qa_chain_self:\n",
    "                self.chat_qa_chain_self[key] = Chat_QA_chain_self(model=model, \n",
    "                                                                   temperature=temperature,\n",
    "                                                                   top_k=top_k, \n",
    "                                                                   chat_history=chat_history, \n",
    "                                                                   persist_path=persist_path, \n",
    "                                                                   embedding=embedding)\n",
    "                chain = self.chat_qa_chain_self[key]\n",
    "                print(self.chat_qa_chain_self)\n",
    "                return \"\", chain.answer(question=question, temperature=temperature, top_k=top_k)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return str(e), chat_history\n",
    "    \n",
    "    def qa_chain_self_answer(self,\n",
    "                                 model:str, \n",
    "                                 embedding = embedding,\n",
    "                                 question:str=None,\n",
    "                                 temperature:float=0.36, \n",
    "                                 appid:str=None,\n",
    "                                 api_key:str=None,\n",
    "                                 api_secret:str=None,\n",
    "                                 persist_path:str='/workspaces/Medical_Chat/WHYembedding/精神科/vector_db',\n",
    "                                 top_k:int=3,\n",
    "                                 chat_history:list=[],\n",
    "                                    ):\n",
    "                            \n",
    "                             \n",
    "        \"\"\"\n",
    "        调用不带历史记录的问答链进行回答\n",
    "        \"\"\" \n",
    "        if question == None or len(question) < 1:\n",
    "            return \"\", chat_history  \n",
    "        try:\n",
    "            key = (model, id(embedding))\n",
    "            if key not in self.qa_chain_self:\n",
    "                self.qa_chain_self[key] = QA_chain_self(model=model, \n",
    "                                                        temperature=temperature,\n",
    "                                                        top_k=top_k, \n",
    "                                                        persist_path=persist_path, \n",
    "                                                        embedding=embedding)\n",
    "                chain = self.qa_chain_self[key]\n",
    "                chat_history.append(\n",
    "                    chain.answer(question=question, temperature=temperature, top_k=top_k)\n",
    "                )\n",
    "                return \"\", chat_history\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return str(e), chat_history\n",
    "\n",
    "    # clear_history 还未测试\n",
    "    def clear_history(self):\n",
    "        if len(self.chat_qa_chain_self) > 0:\n",
    "            for chain in self.chat_qa_chain_self.values():\n",
    "                print(\"清空历史记录\")\n",
    "                chain.clear_history()\n",
    "                \n",
    "def format_chat_prompt(message:str, chat_history:list=[]):\n",
    "    \"\"\"\n",
    "    格式化聊天prompt\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "    for turn in chat_history:\n",
    "        # 从聊天记录中提取用户和机器人的消息。\n",
    "        user_message, bot_message = turn\n",
    "        # 更新 prompt，加入用户和机器人的消息。\n",
    "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
    "    # 将当前的用户消息也加入到 prompt中，并预留一个位置给机器人的回复。\n",
    "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
    "    print(prompt)\n",
    "    # 返回格式化后的 prompt。\n",
    "    return prompt\n",
    "\n",
    "def respond(message,\n",
    "            chat_history,\n",
    "            model,\n",
    "            history_len=3,\n",
    "            temperature=0.36,\n",
    "            max_tokens=2048\n",
    "            ):\n",
    "    \"\"\" \n",
    "    生成普通的llm的回复\n",
    "    \"\"\"\n",
    "    if message == None or len(message) < 1:\n",
    "        return \"请输入问题\", chat_history\n",
    "    try:\n",
    "        # 限制 history 的记忆长度\n",
    "        chat_history = chat_history[-history_len:] if history_len > 0 else []\n",
    "        # 调用上面的函数，将用户的消息和聊天历史记录格式化为一个 prompt。\n",
    "        formatted_prompt = format_chat_prompt(message, chat_history)\n",
    "        # 使用llm对象的predict方法生成机器人的回复（注意：llm对象在此代码中并未定义）。\n",
    "        bot_message = get_completion(model, formatted_prompt)\n",
    "        # 将bot_message中\\n换为<br/>\n",
    "        bot_message = re.sub(r\"\\\\n\", '<br/>', bot_message)\n",
    "        bot_message = re.sub(r\"\\n\", '<br/>', bot_message)\n",
    "        # 将用户的消息和机器人的回复加入到聊天历史记录中。\n",
    "        chat_history.append((message, bot_message))\n",
    "        # 返回一个空字符串和更新后的聊天历史记录（这里的空字符串可以替换为真正的机器人回复，如果需要显示在界面上）。\n",
    "        return \"\", chat_history\n",
    "    except Exception as e:\n",
    "        return e, chat_history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # qa = qa_chain()\n",
    "    # print(qa.chat_qa_chain_self_answer(model=\"Yi-34B-Chat\", question=\"精神病？\", chat_history=[]))\n",
    "    # print(qa.qa_chain_self_answer(model=\"Yi-34B-Chat\", question=\"精神病？\", chat_history=[]))\n",
    "    # print(respond(\"什么是精神病？\", [], \"Yi-34B-Chat\"))\n",
    "    print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/MChat/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://08e35104a2b6926cd7.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://08e35104a2b6926cd7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = qa_chain()\n",
    "import gradio as gr\n",
    "block = gr.Blocks()\n",
    "with block as demo:\n",
    "        with gr.Column(scale=4):\n",
    "            chatbot = gr.Chatbot(height=400, show_copy_button=True, show_share_button=True,)\n",
    "            # 创建一个文本框组件，用于输入 prompt。\n",
    "            msg = gr.Textbox(label=\"Prompt\")\n",
    "\n",
    "            with gr.Row():\n",
    "                # 创建提交按钮。\n",
    "                db_with_his_btn = gr.Button(\"Chat db with history\")\n",
    "                db_wo_his_btn = gr.Button(\"Chat db without history\")\n",
    "                llm_btn = gr.Button(\"Chat with llm\")\n",
    "            with gr.Row():\n",
    "                # 创建一个清除按钮，用于清除聊天机器人组件的内容。\n",
    "                clear = gr.ClearButton(\n",
    "                    components=[chatbot], value=\"Clear console\")\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            file = gr.File(label='请选择知识库目录', file_count='directory',\n",
    "                           file_types=['.txt', '.md', '.docx', '.pdf'])\n",
    "            with gr.Row():\n",
    "                init_db = gr.Button(\"知识库文件向量化\")\n",
    "            model_argument = gr.Accordion(\"参数配置\", open=False)\n",
    "            with model_argument:\n",
    "                temperature = gr.Slider(0,\n",
    "                                        1,\n",
    "                                        value=0.3,\n",
    "                                        step=0.01,\n",
    "                                        label=\"llm temperature\",\n",
    "                                        interactive=True)\n",
    "\n",
    "                top_k = gr.Slider(1,\n",
    "                                  10,\n",
    "                                  value=3,\n",
    "                                  step=1,\n",
    "                                  label=\"vector db search top k\",\n",
    "                                  interactive=True)\n",
    "\n",
    "                history_len = gr.Slider(0,\n",
    "                                        5,\n",
    "                                        value=3,\n",
    "                                        step=1,\n",
    "                                        label=\"history length\",\n",
    "                                        interactive=True)\n",
    "\n",
    "            model_select = gr.Accordion(\"模型选择\")\n",
    "            with model_select:\n",
    "                llm = gr.Dropdown(\n",
    "                    LLM_MODEL_LIST,\n",
    "                    label=\"large language model\",\n",
    "                    value=INIT_MODEL,\n",
    "                    interactive=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gr.close_all()\n",
    "# 启动新的 Gradio 应用，设置分享功能为 True，并使用环境变量 PORT1 指定服务器端口。\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT1']))\n",
    "# 直接启动\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MChat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
